'''
Created on Feb 14, 2016

@author: vitidn
'''
import sys
import itertools

byte_per_int = 4
items_str = ""

def getId(item):
    global items_str
    
    if item not in items_str:
        items_str = items_str + item
    
    return items_str.index(item)
    
def getItem(item_id):
    global items_str
    return items_str[item_id]
    
def getHashIdI(ids,n):
    """
    \n Hash function for the 1st pass
    """
    return (sum(ids)) % n

def getHashIdII(ids,n):
    """
    \n Hash function for the 2nd pass
    """
    return (7*sum(ids)) % n
    
def constructCandidateSets(itemsets):
    """
    \n construct immediate supersets from the current itemsets 
    \n itemsets - format: [frozenset([...]),frozenset([...]),...]
    \n << return >>
    \n final_supersets - construced immediate supersets, format: [frozenset([...]),frozenset([...]),...]
    """
    #no itemsets with size n-1   
    if(len(itemsets)==0):
        return []
        
    size = len(itemsets[0])
    #joining with itself,each itemsets has size n
    #we need supersets with size n+1
    #n(AUB) = n(A) + n(B) - n(A^B)
    #n+1 = n + n - (n-1)
    supersets = [x | y for x in itemsets for y in itemsets if len(x.intersection(y)) == (size-1) ]
    #remove possible duplicate supersets
    supersets = frozenset(supersets)
    final_supersets = []
    #check that immediate subsets of each superset must also appear(also be frequent)
    for superset in supersets:
        is_valid = True
        for subset_candidate in itertools.combinations(superset,size):
            if frozenset(subset_candidate) not in itemsets:
                is_valid = False
                break
        if(is_valid):
            final_supersets.append(superset)
    return final_supersets
    
def multistage_first_pass(f,threshold,freq_itemsets,filter_size,num_bucket):
    """
    \n f - connection stream to the data file
    \n threshold - support threshold
    \n freq_itemsets - the list of all frequent itemsets with size filter_size-1
    \n                 format: [frozenset([...]),frozenset([...]),...]
    \n                 **noted: not sure why in the subsequent rounds you need to keep the count of each itemset?
    \n filter_size - for the 1st round of PYC, this value will be 2(filter pairs for the 1st round)
    \n num_bucket - number of hashed buckets to hash to    
    \n << return >>
    \n freq_itemsets - the list of all frequent itemsets with size filter_size-1. format: [frozenset([...]),frozenset([...]),...]
    \n candidate_itemsets - the list of all itemsets(with size filter_size) generating from freq_itemsets WITHOUT incorporating hash-bucket bitmap
    \n                      *it WILL have value None for the first round of MultiStage
    \n bucket_counts - format: [<row_count>,<row_count>,...]
    
    """
    bucket_counts = [0]*num_bucket
    
    #this is candidate itemsets with size filter_size
    #format: format: [frozenset([...]),frozenset([...]),...]
    candidate_itemsets = []
    
    if(filter_size == 2):
        #for the fist round of 2-Phase PYC, we will count the number of each singleton
        #as well as hashing pairs to a bucket when reading each line. The process is quite
        #different from the rest of other rounds...
        singleton_counts = []
        
        for line in f:
            items = line.replace("\n","").split(",")
            for i in range(0,len(items)):
                #assign id to each item(if it is not already received one)
                index = getId(items[i])
                #count frequency for each singleton
                if(index == len(singleton_counts)):
                    singleton_counts.append(1)
                else:
                    singleton_counts[index] = singleton_counts[index] + 1
                #we will count the frequency of all pairs in particular line concurrently
                for j in range(i+1,len(items)):
                    paired_index = getId(items[j])
                    hash_id = getHashIdI([index,paired_index],num_bucket)
                    bucket_counts[hash_id] = bucket_counts[hash_id] + 1
        
        #in subsequent rounds, freq_itemsets(of size filter_size-1) are already send to the first pass            
        freq_itemsets = [frozenset([x]) for x in range(0,len(singleton_counts)) if singleton_counts[x] >= threshold]
                      
        print("memory for item counts: {0}".format(len(singleton_counts)*byte_per_int*2))       
        print("memory for hash table 1 counts for size {0} itemsets: {1}".format(2,num_bucket*byte_per_int))
        #print content of hash-buckets...    
        print(dict(zip(range(0,len(bucket_counts)),bucket_counts)))
        #frequent itemsets of size filter_size-1 (1) is printed here. Unlike subsequent rounds, it will be printed in the 2nd pass
        #because it is generated by the 2nd pass        
        #this is for printing a list of all frequent itemsets with size filter_size, sort alphabetically
        printed_list = [map(getItem,x) for x in freq_itemsets]
        printed_list = sorted([x[0] for x in printed_list])
        print("frequent itemsets of size {0}: {1}".format(1,printed_list))
        
        #[[DO NOT]] generate candidate_itemsets here for the first round of PYC
        #candidate_itemsets = constructCandidateSets(freq_itemsets)
        
        #stop the process if there is no more candidate itemset to count        
        #if(len(candidate_itemsets)==0):
        #    raise Exception("TerminateAlgorithm")
        
        candidate_itemsets = None
        
        #terminate the process if there is only 0 or 1 singleton(no pair for counting)
        if(len(freq_itemsets)<2):
            raise Exception("TerminateAlgorithm")
            
        return (freq_itemsets,candidate_itemsets,bucket_counts)    
    else:
        candidate_itemsets = constructCandidateSets(freq_itemsets)
        #stop the process if there is no more candidate itemset to count        
        if(len(candidate_itemsets)==0):
            raise Exception("TerminateAlgorithm")
            
        #**noted: not sure why in the subsequent rounds you need to keep the count of each itemset?
        print("memory for frequent itemsets of size {0}: {1}".format(filter_size-1,filter_size*byte_per_int*len(freq_itemsets)))
    
        #read each line and for each line, +1 to a hash-bucket of each candidate itemset it that line contains such candidate
        for line in f:
            items = line.replace("\n","").split(",")
            item_ids = [getId(x) for x in items]
            item_set = frozenset(item_ids)
            for candidate_itemset in candidate_itemsets:
                if candidate_itemset.issubset(item_set):
                    hash_id = getHashIdI(candidate_itemset,num_bucket)
                    bucket_counts[hash_id] = bucket_counts[hash_id] + 1
        
    
    print("memory for hash table counts for size {0} itemsets: {1}".format(filter_size,num_bucket*byte_per_int))    
    #print content of hash-buckets...    
    print(dict(zip(range(0,len(bucket_counts)),bucket_counts)))
    return (freq_itemsets,candidate_itemsets,bucket_counts)
 
def multistage_second_pass(f,threshold,freq_itemsets,candidate_itemsets,bucket_counts,filter_size,num_bucket):
    """
    \n candidate_itemsets - for the first round, this value will be None
    \n << return >>
    \n - freq_itemsets
    \n - candidate_itemsets *it WILL have value None for the first round of MultiStage
    \n - bitmap_bucket_counts - bitmap for hashI
    \n - bucket2_counts - hash-buckets counts for hashII
    """
    #print memory used for frequent singletons as it is not printed before...
    if(filter_size==2):
        print("memory for frequent itemsets of size {0}: {1}".format(1,len(freq_itemsets)*byte_per_int*2))

    #turn hash-bucktes into bitmap    
    bitmap_bucket_counts = [1 if x >= threshold else 0 for x in bucket_counts ]
    print("bitmap 1 size:{0}".format(len(bitmap_bucket_counts)))
    
    #now, we incorporate bitmap of hash-buckets
    #dict_candidate_itemsets only stores itemsets that hashed to frequent hash-buckets
    #format {frozenset(i,j):<count>,frozenset(i,k):<count>,...}
    dict_candidate_itemsets = {}
    if(filter_size == 2):
        #candidate_itemsets is not sent for the first round
        #dict_candidate_itemsets will be generated along while reading frequent pair (i,j) and verfyting that the pair
        #hashed to a frequent hash-bucket
        for i in range(0,len(freq_itemsets)):
            for j in range(i+1,len(freq_itemsets)):
                #subscript [0] because they are frozenset([<value>]) 
                x = list(freq_itemsets[i])[0]
                y = list(freq_itemsets[j])[0]
                hash_id = getHashIdI([x,y],num_bucket)
                if(bitmap_bucket_counts[hash_id] == 1):
                    #initial couting as 0. It will be increment when reading the actual file
                    dict_candidate_itemsets[frozenset([x,y])] = 0

    else:
        #for subsequent rounds, candidate_itemset is sent from the first pass
        for candidate_itemset in candidate_itemsets:
            hash_id = getHashIdI(candidate_itemset,num_bucket)
            if(bitmap_bucket_counts[hash_id] == 1):
                #initial couting as 0. It will be increment when reading the actual file
                dict_candidate_itemsets[candidate_itemset] = 0
    
    bucket2_counts = [0]*num_bucket 
    #now, each itemset in dict_candidate_itemsets is (1)frequent itemsets (2)hashed to frequent hashI bucket    
    #read each line and for each line, +1 to a hash-bucket2 of each candidate itemset it that line contains such candidate
    for line in f:
        items = line.replace("\n","").split(",")
        item_ids = [getId(x) for x in items]
        item_set = frozenset(item_ids)
        for candidate_itemset in dict_candidate_itemsets:
            if candidate_itemset.issubset(item_set):
                hash_id = getHashIdII(candidate_itemset,num_bucket)
                bucket2_counts[hash_id] = bucket2_counts[hash_id] + 1
        
    
    print("memory for hash table 2 counts for size {0} itemsets: {1}".format(filter_size,num_bucket*byte_per_int))    
    #print content of hash-buckets...    
    print(dict(zip(range(0,len(bucket2_counts)),bucket2_counts)))
    
    return (freq_itemsets,candidate_itemsets,bitmap_bucket_counts,bucket2_counts)        

def multistage_third_pass(f,threshold,freq_itemsets,candidate_itemsets,bitmap_bucket_counts,bucket2_counts,num_bucket):
    """
    \n candidate_itemsets - for the first round, this value will be None
    """
    #print memory used for frequent singletons as it is not printed before...
    if(filter_size==2):
        print("memory for frequent itemsets of size {0}: {1}".format(1,len(freq_itemsets)*byte_per_int*2))
        
    #turn hash-bucktes2 into bitmap2    
    bitmap2_bucket_counts = [1 if x >= threshold else 0 for x in bucket2_counts ]
    
    print("bitmap 1 size:{0}".format(len(bitmap_bucket_counts)))
    print("bitmap 2 size:{0}".format(len(bitmap2_bucket_counts)))
    
    #now, we incorporate bitmap & bitmap2 of hash-buckets
    #dict_candidate_itemsets only stores itemsets that hashed to frequent hash-buckets
    #format {frozenset(i,j):<count>,frozenset(i,k):<count>,...}
    dict_candidate_itemsets = {}
    if(filter_size == 2):
        #candidate_itemsets is not sent for the first round
        #dict_candidate_itemsets will be generated along while reading frequent pair (i,j) and verfyting that the pair
        #hashed to a frequent hash-bucket1 & frequent hash-bucket2
        for i in range(0,len(freq_itemsets)):
            for j in range(i+1,len(freq_itemsets)):
                #subscript [0] because they are frozenset([<value>]) 
                x = list(freq_itemsets[i])[0]
                y = list(freq_itemsets[j])[0]
                hash_id1 = getHashIdI([x,y],num_bucket)
                hash_id2 = getHashIdII([x,y],num_bucket)
                if(bitmap_bucket_counts[hash_id1] == 1 and bitmap2_bucket_counts[hash_id2] == 1):
                    #initial couting as 0. It will be increment when reading the actual file
                    dict_candidate_itemsets[frozenset([x,y])] = 0

    else:
        #for subsequent rounds, candidate_itemset is sent from the first pass
        for candidate_itemset in candidate_itemsets:
            hash_id1 = getHashIdI(candidate_itemset,num_bucket)
            hash_id2 = getHashIdII(candidate_itemset,num_bucket)
            if(bitmap_bucket_counts[hash_id1] == 1 and bitmap2_bucket_counts[hash_id2] == 1):
                #initial couting as 0. It will be increment when reading the actual file
                dict_candidate_itemsets[candidate_itemset] = 0
        
    print("memory for candidates counts of size {0}: {1}".format(filter_size,(filter_size+1)*len(dict_candidate_itemsets)*byte_per_int))

    #do the actual count of each candidate itemset
    for line in f:
        items = line.replace("\n","").split(",")
        item_ids = [getId(x) for x in items]
        item_set = frozenset(item_ids)
        for candidate_filter_itemset in dict_candidate_itemsets:
            if(candidate_filter_itemset.issubset(item_set)):
                dict_candidate_itemsets[candidate_filter_itemset] = dict_candidate_itemsets[candidate_filter_itemset] + 1
    
    #filter candidate_itemsets and now we get frequent itemsets of size filter_size 
    true_freq_itemsets = [itemset for itemset,count in dict_candidate_itemsets.items() if count >= threshold]
            
    #this is for printing a list of all frequent itemsets with size filter_size, sort alphabetically
    printed_list = sorted([sorted(map(getItem,x)) for x in true_freq_itemsets])
    print("frequent itemsets of size {0}: {1}".format(filter_size,printed_list))
    
    return true_freq_itemsets
    
if __name__ == "__main__":
    filename = sys.argv[1]
    frequent_threshold = int(sys.argv[2])
    num_bucket = int(sys.argv[3])
    
    freq_itemsets = []
    filter_size = 2
    
    while True:
        try:
            result = multistage_first_pass(open(filename),frequent_threshold,freq_itemsets,filter_size,num_bucket)
            freq_itemsets,candidate_itemsets,bucket_counts = result[0],result[1],result[2]
            print("")
            result = multistage_second_pass(open(filename),frequent_threshold,freq_itemsets,candidate_itemsets,bucket_counts,filter_size,num_bucket)
            freq_itemsets,candidate_itemsets,bitmap_bucket_counts,bucket2_counts = result[0],result[1],result[2],result[3]
            print("")
            freq_itemsets = multistage_third_pass(open(filename), frequent_threshold, freq_itemsets, candidate_itemsets, bitmap_bucket_counts, bucket2_counts,num_bucket)
            print("")
        except :
            break;
        filter_size = filter_size + 1
